{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Problem 4:\n",
    "\n",
    "## List all algorithms and methods that we have covered in this course. Write 3 sentences to describe what each algorithm and method solves etc.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dijkstra's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an algorithm for finding the shortest paths between nodes in a graph. It can be used to find a shortest-path tree to all nodes in a graph starting from a source node. A variant of Dijkstra's algorithm is known as uniform-cost search."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kruskal's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm that finds the edge with the least possible weight that connects any two nodes in a graph. It is a greedy algorithm that finds the minimum spanning tree of a connected, weighted graph by first selecting the lowest cost edge in the graph, then the next lowest cost edge that doesn't produce a cycle, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prim's Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a greedy minimum-spanning-tree algorithm. It is used on weighted, undirected graphs. The algorithm builds the spanning tree one vertex at a time, from an arbitrary starting vertex, at each step adding the lowest cost edge from the tree to another vertex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Depth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expands the leftmost side of a tree, all the way to the deepest node, and then backtracks to work its way rightward across the tree until the solution is found. It always finds the leftmost solution, regardless of depth or cost. It is not an optimal algorithm, especially if the solution is on the right side of the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breadth-First Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expands the shallowest part of the tree first, working left to right and then downward, until a solution is found. It always finds the shallowest solution, not necessarily the best solution. It is not an optimal algorithm if cost is taken into account, as it will always find the first solution in the tree from top to bottom, regardless of its cost. If the best solution is deeper in the tree, BFS will not find it if there is another solution above it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uniform-Cost Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes all nodes with costs less than the cheapest solution. It is a complete and optimal algorith. Explores nodes in every direction without information about goal location. It is the best algorithm for a search problem which does not involve the use of heuristics. It searches in branches that are more or less the same in cost. It can solve any general graph for optimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uses a search heuristic to always select the next node which costs the least, is the closest, gives the highest reward, etc. It selects the locally optimal choice at each stage with the intent of finding a global optimum. In many problems, a greedy algorithm does not usually produce an overall optimal solution, but will always yield locally optimal solutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A* Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An algorithm used in pathfinding and graph traversal. It has very high performance and accuracy. Combines uniform-cost and greedy search algorithms. Uses backwards costs and estimates of forward costs to determine which node to expand next. Expands nodes based on increasing total f value (f-contours). It is an optimal algorithm if its heuristic is admissible. For every state s, nodes that reach s optimally are expanded before nodes that reach s suboptimally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A subsection of Tree Search which doesn't expand redundant states. Before expanding a node, check to see that it has never been expanded before. If it has, skip it and don't expand it again. If it is a new node, add it to the closed list of nodes to not expand again, and expand the node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimax Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A state-space search tree in which players alternate turns and have perfect information about their opponent's moves. Each node's \"minimax\" value is computed based on the best achievable utility against a rational/optimal opponent. Implementation is similar to an exhaustive DFS. Minimax can be customized with depth-limitations as well as Alpha-Beta Pruning, to reduce the number of nodes that are explored."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expectimax Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to Minimax Search, except calculates the average score under optimal play by using max nodes and chance nodes. Outcomes depend on a combination of players' optimal moves and chance elements. Chance nodes calculate expected utility based on probability, and max nodes are just like the max nodes in Minimax search. It can be depth-limited and pruned as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov Decision Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provides a framework for modeling decision making where outcomes are partially random and partially under the control of a decision maker. Is defined by a set of states S, a set of actions A, a transition function T(s, a, s'), a reward function R(s, a, s'), a start state, and optionally a terminal state. An MDP is a non-deterministic search problem. One way to an MDP is with Expectimax search. \"Markov\" means that action outcomes depend only on the current state; the future and past are independent. MDPs try to find an optimal policy, giving an action for each state, that maximizes utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Bellman Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equations which represent a necessary condition for optimality. They write the value of a decision problem at a certain point in time in terms of the payoff from some initial choices and the value of the remaining decision problem that results from those initial choices. This breaks a dynamic optimization problem into a sequence of simpler subproblems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Passive) Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a fixed policy, learn the transitions and rewards so that you can learn the state values. Methods of passive reinforcement learning include direct utility estimation, adaptive dynamic programming, and temporal difference learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Active) Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You don't know the transitions or rewards. You choose the actions. The goal is to learn the optimal policy/values. This is referred to as \"online planning\" as opposed to \"offline planning\". Methods of active reinforcement learning include adaptive dynamic programming with exploration function, and Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-learning is a temporal difference learning method which does not require the agent to learn the transitional model. Instead it learns Q-value functions Q(s, a).\n",
    "\n",
    "U(s) = max_a Q(s, a)\n",
    "\n",
    "Q-learning is simpler to compute than adaptive dynamic programming, but it is also slower. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn policies that maximize rewards, not the values that predict them. Start with an ok solution (e.g. Q-learning) and fine-tune it by adjusting value weights. Many sample episodes are required to be run in order to determine if a policy has become better or worse. If there are a lot of features, it can be impractical to use policy search."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
